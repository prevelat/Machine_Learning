{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DRIVE_ONLY_Training.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prevelat/Machine_Learning/blob/master/DRIVE_ONLY_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1u9QVVsShC9X"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KEHR2Ui-lo8O",
        "colab": {}
      },
      "source": [
        "# Note: If you haven't installed the following dependencies, run:\n",
        "!apt-get install -y xvfb\n",
        "!pip install 'gym==0.10.11'\n",
        "!pip install 'imageio==2.4.0'\n",
        "!pip install PILLOW\n",
        "!pip install 'pyglet==1.3.2'\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install tf-agents\n",
        "!pip install tensorflow==2.0.0\n",
        "!pip install tensorflow-probability==0.8\n",
        "try:\n",
        "  %%tensorflow_version 2.x\n",
        "except:\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sMitx5qSgJk1",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import base64\n",
        "import imageio\n",
        "import IPython\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL.Image\n",
        "import pyvirtualdisplay\n",
        "import numpy as np\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.drivers import dynamic_step_driver\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.eval import metric_utils\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.networks import q_network\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.utils import common\n",
        "from tf_agents.policies import policy_saver\n",
        "\n",
        "tf.compat.v1.enable_v2_behavior()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NspmzG4nP3b9",
        "colab": {}
      },
      "source": [
        "tf.version.VERSION"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6aoHFNNFxtp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "os.chdir(\"drive/My Drive/Colab Notebooks\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycQWqJaSFylu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "policy_drive_folder = 'Policies/'\n",
        "data_drive_folder = 'Data/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LmC0NDhdLIKY"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HC1kNrOsLSIZ",
        "colab": {}
      },
      "source": [
        "num_iterations = 1000 # @param {type:\"integer\"}\n",
        "\n",
        "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
        "\n",
        "batch_size = 64  # @param {type:\"integer\"}\n",
        "learning_rate = 1e-3  # @param {type:\"number\"}\n",
        "\n",
        "num_eval_episodes = 100  # @param {type:\"integer\"}\n",
        "eval_interval =   250# @param {type:\"integer\"}\n",
        "\n",
        "collect_steps = 500  # @param {type:\"integer\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOfLj0PSh30T",
        "colab_type": "text"
      },
      "source": [
        "### Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byu43nxzcYIq",
        "colab_type": "text"
      },
      "source": [
        "Load the CartPole environment from the OpenAI Gym suite. <br/>\n",
        "Usually two environments are instantiated: one for training and one for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pYEz-S9gEv2-",
        "colab": {}
      },
      "source": [
        "env_name = 'CartPole-v0'\n",
        "env = suite_gym.load(env_name)\n",
        "train_py_env = suite_gym.load(env_name)\n",
        "eval_py_env = suite_gym.load(env_name)\n",
        "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
        "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDanNpi4igIg",
        "colab_type": "text"
      },
      "source": [
        "### Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TgkdEPg_muzV",
        "colab": {}
      },
      "source": [
        "fc_layer_params = (100,)\n",
        "\n",
        "q_net = q_network.QNetwork(\n",
        "    train_env.observation_spec(),\n",
        "    train_env.action_spec(),\n",
        "    fc_layer_params=fc_layer_params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfi_g3C0iiRE",
        "colab_type": "text"
      },
      "source": [
        "Now use `tf_agents.agents.dqn.dqn_agent` to instantiate a `DqnAgent`. In addition to the `time_step_spec`, `action_spec` and the QNetwork, the agent constructor also requires an optimizer (in this case, `AdamOptimizer`), a loss function, and an integer step counter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jbY4yrjTEyc9",
        "colab": {}
      },
      "source": [
        "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "\n",
        "train_step_counter = tf.Variable(0)\n",
        "\n",
        "agent = dqn_agent.DqnAgent(\n",
        "    train_env.time_step_spec(),\n",
        "    train_env.action_spec(),\n",
        "    q_network=q_net,\n",
        "    optimizer=optimizer,\n",
        "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
        "    train_step_counter=train_step_counter)\n",
        "\n",
        "agent.initialize()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiAuM0fRjK_2",
        "colab_type": "text"
      },
      "source": [
        "### Replay Buffer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rL1PBa6EeA3t",
        "colab_type": "text"
      },
      "source": [
        "The replay buffer keeps track of data collected from the environment. This tutorial uses `tf_agents.replay_buffers.tf_uniform_replay_buffer.TFUniformReplayBuffer`, as it is the most common. \n",
        "\n",
        "The constructor requires the specs for the data it will be collecting. This is available from the agent using the `collect_data_spec` method. The batch size and maximum buffer length are also required."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vX2zGUWJGWAl",
        "colab": {}
      },
      "source": [
        "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "    data_spec=agent.collect_data_spec,\n",
        "    batch_size=train_env.batch_size,\n",
        "    max_length=replay_buffer_max_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CA0XPccljQ3T",
        "colab_type": "text"
      },
      "source": [
        "For most agents, collect_data_spec is a named tuple called Trajectory, containing the specs for observations, actions, rewards, and other items."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_IZ-3HcqgE1z",
        "colab": {}
      },
      "source": [
        "agent.collect_data_spec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sy6g1tGcfRlw",
        "colab": {}
      },
      "source": [
        "agent.collect_data_spec._fields"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixPSaaj942yW",
        "colab_type": "text"
      },
      "source": [
        "### Read data from file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6LgtwsXwsro",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_older_data(last_data, data_list):\n",
        "  for name in data_list:\n",
        "    if last_data != int(name.split('.')[0]):\n",
        "      !mv $name $data_drive_folder\n",
        "\n",
        "def get_data_drive(data_file_number):\n",
        "  print('----------FETCHING DATA')\n",
        "  last_data = 0\n",
        "  # all_data = !ls $data_drive_folder\n",
        "  all_data = !ls\n",
        "  data_list = list()\n",
        "  for name in all_data:\n",
        "    if '.data.' in name:\n",
        "      name = name[1:]\n",
        "      data_list.append(name)\n",
        "      s = name.split('.')\n",
        "      if int(s[0]) > int(data_file_number):\n",
        "        data_file_number = int(s[0])\n",
        "        last_data = int(s[0])\n",
        "        file_name = name\n",
        "  if last_data == 0:\n",
        "    print('----------NO NEW DATA, TRYING AGAIN...')\n",
        "    time.sleep(10)\n",
        "    return get_data_drive(data_file_number)\n",
        "  else:\n",
        "    save_older_data(last_data, data_list)\n",
        "    print('----------DATA READY')\n",
        "    # file_name = data_drive_folder + file_name\n",
        "    return file_name, data_file_number"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAIE-l125AVF",
        "colab_type": "text"
      },
      "source": [
        "### Convert data back to Trajectory and add to buffer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dijeHpf31Eow",
        "colab": {}
      },
      "source": [
        "def add_data_to_replay_buffer(buffer, data):\n",
        "  for i in data:\n",
        "\n",
        "    step_type = tf.convert_to_tensor(data[i]['step_type'], tf.int32)\n",
        "    observation = tf.convert_to_tensor(data[i]['observation'], tf.float32)\n",
        "    action = tf.convert_to_tensor(data[i]['action'], tf.int64)\n",
        "    policy_info = tuple(data[i]['policy_info'])\n",
        "    next_step_type = tf.convert_to_tensor(data[i]['next_step_type'], tf.int32)\n",
        "    reward = tf.convert_to_tensor(data[i]['reward'], tf.float32)\n",
        "    discount = tf.convert_to_tensor(data[i]['discount'], tf.float32)\n",
        "\n",
        "    traj = trajectory.Trajectory(step_type, observation, action, policy_info, next_step_type, reward, discount)\n",
        "    buffer.add_batch(traj)\n",
        "  print('Replay buffer ready')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-fXlWzn4xna",
        "colab_type": "text"
      },
      "source": [
        "The replay buffer is now a collection of Trajectories."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9lX70375dQ3",
        "colab_type": "text"
      },
      "source": [
        "The agent needs access to the replay buffer. This is provided by creating an iterable `tf.data.Dataset` pipeline which will feed data to the agent.\n",
        "\n",
        "Each row of the replay buffer only stores a single observation step. But since the DQN Agent needs both the current and next observation to compute the loss, the dataset pipeline will sample two adjacent rows for each item in the batch (`num_steps=2`).\n",
        "\n",
        "This dataset is also optimized by running parallel calls and prefetching data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28SFjbph4CYJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvL99M714GT6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# iterator = iter(dataset)\n",
        "# print(iterator)\n",
        "# # For the curious:\n",
        "# # Uncomment to see what the dataset iterator is feeding to the agent.\n",
        "# # Compare this representation of replay data \n",
        "# # to the collection of individual trajectories shown earlier.\n",
        "\n",
        "# # iterator.next()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3S8NRY354yc",
        "colab_type": "text"
      },
      "source": [
        "### Training Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBPXjC6qdeXR",
        "colab_type": "text"
      },
      "source": [
        "#### Metrics and Evaluation\n",
        "\n",
        "The most common metric used to evaluate a policy is the average return. The return is the sum of rewards obtained while running a policy in an environment for an episode. Several episodes are run, creating an average return.\n",
        "\n",
        "The following function computes the average return of a policy, given the policy, environment, and a number of episodes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xl2r9yx4dp0S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@test {\"skip\": true}\n",
        "def compute_avg_return(environment, policy, num_episodes=10):\n",
        "\n",
        "  total_return = 0.0\n",
        "  for _ in range(num_episodes):\n",
        "\n",
        "    time_step = environment.reset()\n",
        "    episode_return = 0.0\n",
        "\n",
        "    while not time_step.is_last():\n",
        "      action_step = policy.action(time_step)\n",
        "      time_step = environment.step(action_step.action)\n",
        "      episode_return += time_step.reward\n",
        "    total_return += episode_return\n",
        "\n",
        "  avg_return = total_return / num_episodes\n",
        "  return avg_return.numpy()[0]\n",
        "\n",
        "\n",
        "# See also the metrics module for standard implementations of different metrics.\n",
        "# https://github.com/tensorflow/agents/tree/master/tf_agents/metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITybcFOLdqym",
        "colab_type": "text"
      },
      "source": [
        "#### Training\n",
        "\n",
        "-   use data to train the agent's neural network(s)\n",
        "\n",
        "This example also periodicially evaluates the policy and prints the current score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGRTValp6De-",
        "colab_type": "text"
      },
      "source": [
        "### Visualization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSL5-LSp6G-x",
        "colab_type": "text"
      },
      "source": [
        "#### Plots\n",
        "\n",
        "Use `matplotlib.pyplot` to chart how the policy improved during training.\n",
        "\n",
        "One iteration of `Cartpole-v0` consists of 200 time steps. The environment gives a reward of `+1` for each step the pole stays up, so the maximum return for one episode is 200. The charts shows the return increasing towards that maximum each time it is evaluated during training. (It may be a little unstable and not increase monotonically each time.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FJrvBkGZw6K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def visualization(num_iterations, eval_interval, returns):\n",
        "  iterations = range(0, num_iterations + 1, eval_interval)\n",
        "  plt.plot(iterations, returns)\n",
        "  plt.ylabel('Average Return')\n",
        "  plt.xlabel('Iterations')\n",
        "  plt.ylim(top=250)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOBtdlLsYhsH",
        "colab_type": "text"
      },
      "source": [
        "### Policy Saver"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RVeFeOl_fY0f",
        "colab": {}
      },
      "source": [
        "def generate_policy_folder_name(avg, ID):\n",
        "  i = datetime.datetime.now() - datetime.datetime(1970,1,1)\n",
        "  folder_name = str(avg) + '.policy.' + str(i.total_seconds()) \n",
        "  return folder_name"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Z40v_jVLd0Fg"
      },
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYzLyOBx4WqB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trajectory_trimmer(data, avg):\n",
        "  one_ep = list()\n",
        "  reward = 0\n",
        "  ref = avg\n",
        "  if ref > 100:\n",
        "    ref = 100\n",
        "  for i in range(collect_steps - 1):\n",
        "    reward += data[i]['reward']\n",
        "    one_ep.append(i)\n",
        "    if data[i]['step_type'] == 2:\n",
        "      if reward < ref:\n",
        "        for traj in one_ep:\n",
        "          del data[traj]\n",
        "      one_ep.clear()\n",
        "      reward = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGPTFaXEdSeB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_file_number = 0\n",
        "best_avg = 0\n",
        "ID = 0\n",
        "while True:\n",
        "\n",
        "  # Fetch new data\n",
        "  data_name, data_file_number = get_data_drive(data_file_number)\n",
        "  data_read = pd.read_json(data_name)\n",
        "  # !rm $data_name\n",
        "  trajectory_trimmer(data_read, best_avg)\n",
        "\n",
        "  # Feed replay buffer and ready dataset for training\n",
        "  replay_buffer.clear()\n",
        "  add_data_to_replay_buffer(replay_buffer, data_read)\n",
        "  dataset = replay_buffer.as_dataset(\n",
        "      num_parallel_calls=3, \n",
        "      sample_batch_size=batch_size, \n",
        "      num_steps=2).prefetch(3)\n",
        "  iterator = iter(dataset)\n",
        "\n",
        "  # Train and Eval\n",
        "\n",
        "  ## (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
        "  agent.train = common.function(agent.train)\n",
        "\n",
        "  ## Reset the train step\n",
        "  agent.train_step_counter.assign(0)\n",
        "\n",
        "  ## Evaluate the agent's policy once before training.\n",
        "  avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "  print ('Evaluate the agent\\'s policy once before training - ' ,avg_return)\n",
        "  returns = [avg_return]\n",
        "  try:\n",
        "    print('Last best average return = ', best_avg)\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "  policy_list = dict()\n",
        "\n",
        "  print('--------------------------------------')\n",
        "  for _ in range(num_iterations):\n",
        "\n",
        "    # Sample a batch of data from the buffer and update the agent's network.\n",
        "    experience, unused_info = next(iterator)\n",
        "    train_loss = agent.train(experience).loss\n",
        "\n",
        "    step = agent.train_step_counter.numpy()\n",
        "\n",
        "    if step % 50 == 0:\n",
        "      print('step = {0}: loss = {1}'.format(step, train_loss))\n",
        "\n",
        "    if step % eval_interval == 0:\n",
        "      avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "      print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
        "      # returns.append(avg_return)\n",
        "      if avg_return not in policy_list:\n",
        "        policy_list[avg_return] = policy_saver.PolicySaver(agent.policy, batch_size=None)\n",
        "  print('--------------------------------------')\n",
        "\n",
        "  ## Visualization\n",
        "  # visualization(num_iterations, eval_interval, returns)\n",
        "\n",
        "  # Save policy to drive\n",
        "  avg_bested = False\n",
        "  for avg in policy_list:\n",
        "    if avg > best_avg:\n",
        "      avg_bested = True\n",
        "      best_avg = avg\n",
        "  if avg_bested:\n",
        "    saver = policy_list[best_avg]\n",
        "    policy_folder_name = generate_policy_folder_name(best_avg, ID)\n",
        "    saver.save(policy_folder_name)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}