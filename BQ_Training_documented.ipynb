{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BQ_Training_documented.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prevelat/Machine_Learning/blob/master/BQ_Training_documented.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1u9QVVsShC9X"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FavGTJZNeigt",
        "colab_type": "text"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KEHR2Ui-lo8O",
        "colab": {}
      },
      "source": [
        "# Note: If you haven't installed the following dependencies, run:\n",
        "!apt-get install -y xvfb\n",
        "!pip install 'gym==0.10.11'\n",
        "!pip install 'imageio==2.4.0'\n",
        "!pip install PILLOW\n",
        "!pip install 'pyglet==1.3.2'\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install tf-agents\n",
        "!pip install tensorflow==2.0.0\n",
        "!pip install tensorflow-probability==0.8\n",
        "try:\n",
        "  %%tensorflow_version 2.x\n",
        "except:\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sMitx5qSgJk1",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import base64\n",
        "import imageio\n",
        "import IPython\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL.Image\n",
        "import pyvirtualdisplay\n",
        "import numpy as np\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import time\n",
        "import tensorflow as tf\n",
        "\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.drivers import dynamic_step_driver\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.eval import metric_utils\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.networks import q_network\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.utils import common\n",
        "from tf_agents.policies import policy_saver\n",
        "\n",
        "%load_ext tensorboard\n",
        "\n",
        "tf.compat.v1.enable_v2_behavior()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NspmzG4nP3b9",
        "colab": {}
      },
      "source": [
        "tf.version.VERSION"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xXcLWEgeo3f",
        "colab_type": "text"
      },
      "source": [
        "### Cloud Management"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6aoHFNNFxtp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Google Cloud Setup\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "try:\n",
        "  os.chdir(\"drive/My Drive/Colab Notebooks\")\n",
        "except:\n",
        "  pass\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "print('Authenticated')\n",
        "\n",
        "from google.cloud import bigquery"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCPA_V4YmXED",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Google Cloud Variables\n",
        "\n",
        "project_id = 'ml-piscine-262622'\n",
        "dataset_id = 'ml_piscine_bq'\n",
        "\n",
        "environment_table_id = dataset_id + '.environment'\n",
        "agent_policy_table_id = dataset_id + '.agent_policy_v2'\n",
        "observation_table_id = dataset_id + '.observation'\n",
        "episodes_table_id = dataset_id + '.episodes'\n",
        "steps_table_id = dataset_id + '.steps'\n",
        "traj_raw_table_id = dataset_id + '.traj_raw'\n",
        "\n",
        "client = bigquery.Client(project=project_id)\n",
        "try:\n",
        "  dataset = bigquery.Dataset(dataset_id)\n",
        "  dataset.location = \"US\"\n",
        "  dataset = client.create_dataset(dataset)\n",
        "except:\n",
        "  pass\n",
        "\n",
        "!gcloud config set project {project_id}\n",
        "policy_bucket_folder = 'gs://ml-piscine-bucket/policy/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LmC0NDhdLIKY"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HC1kNrOsLSIZ",
        "colab": {}
      },
      "source": [
        "num_iterations = 200 # @param {type:\"integer\"}\n",
        "\n",
        "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
        "\n",
        "batch_size = 64  # @param {type:\"integer\"}\n",
        "learning_rate = 1e-3  # @param {type:\"number\"}\n",
        "\n",
        "num_eval_episodes = 100  # @param {type:\"integer\"}\n",
        "eval_interval =   50# @param {type:\"integer\"}\n",
        "\n",
        "collect_steps = 1000  # @param {type:\"integer\"}\n",
        "\n",
        "dqn_layer_params = (100, )  # @param"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOfLj0PSh30T",
        "colab_type": "text"
      },
      "source": [
        "### Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byu43nxzcYIq",
        "colab_type": "text"
      },
      "source": [
        "Load the CartPole environment from the OpenAI Gym suite. <br/>\n",
        "Usually two environments are instantiated: one for training and one for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pYEz-S9gEv2-",
        "colab": {}
      },
      "source": [
        "env_name = 'CartPole-v0'\n",
        "env = suite_gym.load(env_name)\n",
        "train_py_env = suite_gym.load(env_name)\n",
        "eval_py_env = suite_gym.load(env_name)\n",
        "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
        "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDanNpi4igIg",
        "colab_type": "text"
      },
      "source": [
        "### Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrKCkSOniSkQ",
        "colab_type": "text"
      },
      "source": [
        "The DQN agent can be used in any environment which has a discrete action space.\n",
        "\n",
        "At the heart of a DQN Agent is a QNetwork, a neural network model that can learn to predict QValues (expected returns) for all actions, given an observation from the environment.\n",
        "\n",
        "Use tf_agents.networks.q_network to create a QNetwork, passing in the observation_spec, action_spec, and a tuple describing the number and size of the model's hidden layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TgkdEPg_muzV",
        "colab": {}
      },
      "source": [
        "fc_layer_params = dqn_layer_params\n",
        "\n",
        "q_net = q_network.QNetwork(\n",
        "    train_env.observation_spec(),\n",
        "    train_env.action_spec(),\n",
        "    fc_layer_params=fc_layer_params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfi_g3C0iiRE",
        "colab_type": "text"
      },
      "source": [
        "Now use `tf_agents.agents.dqn.dqn_agent` to instantiate a `DqnAgent`. In addition to the `time_step_spec`, `action_spec` and the QNetwork, the agent constructor also requires an optimizer (in this case, `AdamOptimizer`), a loss function, and an integer step counter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jbY4yrjTEyc9",
        "colab": {}
      },
      "source": [
        "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "\n",
        "train_step_counter = tf.Variable(0)\n",
        "\n",
        "loss_fn = common.element_wise_squared_loss\n",
        "\n",
        "agent = dqn_agent.DqnAgent(\n",
        "    train_env.time_step_spec(),\n",
        "    train_env.action_spec(),\n",
        "    q_network=q_net,\n",
        "    optimizer=optimizer,\n",
        "    td_errors_loss_fn=loss_fn,\n",
        "    train_step_counter=train_step_counter)\n",
        "\n",
        "agent.initialize()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5unb-hqJB8b",
        "colab_type": "text"
      },
      "source": [
        "Big Query agent table schema:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTRzW-QNpvvc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "agent_policy_schema = [\n",
        "  {'name': 'ID', 'type': 'INTEGER', 'mode': 'REQUIRED'},\n",
        "  {'name': 'source', 'type': 'STRING', 'mode': 'REQUIRED'},\n",
        "  {'name': 'avg_return', 'type': 'FLOAT', 'mode': 'REQUIRED'},\n",
        "  {'name': 'eval_episodes', 'type': 'INTEGER', 'mode': 'REQUIRED'},\n",
        "  {'name': 'training_steps', 'type': 'INTEGER', 'mode': 'REQUIRED'},\n",
        "  {'name': 'optimizer', 'type': 'STRING', 'mode': 'REQUIRED'},\n",
        "  {'name': 'loss_fn', 'type': 'STRING', 'mode': 'REQUIRED'},\n",
        "  {'name': 'learning_rate', 'type': 'FLOAT', 'mode': 'REQUIRED'},\n",
        "  {'name': 'dqn_layer_params', 'type': 'STRING', 'mode': 'REQUIRED'}\n",
        "]\n",
        "ag_pol_d = dict()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiAuM0fRjK_2",
        "colab_type": "text"
      },
      "source": [
        "### Replay Buffer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rL1PBa6EeA3t",
        "colab_type": "text"
      },
      "source": [
        "The replay buffer keeps track of data collected from the environment. This tutorial uses `tf_agents.replay_buffers.tf_uniform_replay_buffer.TFUniformReplayBuffer`, as it is the most common. \n",
        "\n",
        "The constructor requires the specs for the data it will be collecting. This is available from the agent using the `collect_data_spec` method. The batch size and maximum buffer length are also required."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vX2zGUWJGWAl",
        "colab": {}
      },
      "source": [
        "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "    data_spec=agent.collect_data_spec,\n",
        "    batch_size=train_env.batch_size,\n",
        "    max_length=replay_buffer_max_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CA0XPccljQ3T",
        "colab_type": "text"
      },
      "source": [
        "For most agents, collect_data_spec is a named tuple called Trajectory, containing the specs for observations, actions, rewards, and other items."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_IZ-3HcqgE1z",
        "colab": {}
      },
      "source": [
        "agent.collect_data_spec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sy6g1tGcfRlw",
        "colab": {}
      },
      "source": [
        "agent.collect_data_spec._fields"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAIE-l125AVF",
        "colab_type": "text"
      },
      "source": [
        "Convert data back to Trajectory and add to buffer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dijeHpf31Eow",
        "colab": {}
      },
      "source": [
        "def add_data_to_replay_buffer(buffer, data):\n",
        "\n",
        "  \"\"\"\n",
        "  Input new data to the replay buffer\n",
        "  Arguments:\n",
        "      buffer : replay buffer that the new data should be added\n",
        "      data : data that should be added to the replay buffer\n",
        "  \"\"\"\n",
        "\n",
        "  for i in data:\n",
        "\n",
        "    obs = [data[i]['obs0'], data[i]['obs1'], data[i]['obs2'], data[i]['obs3']]\n",
        "\n",
        "    step_type = tf.convert_to_tensor(data[i]['step_type'], tf.int32)\n",
        "    observation = tf.convert_to_tensor(obs, tf.float32)\n",
        "    action = tf.convert_to_tensor(data[i]['action'], tf.int64)\n",
        "    # policy_info = tuple(data[i]['policy_info'])\n",
        "    policy_info = ()\n",
        "    next_step_type = tf.convert_to_tensor(data[i]['next_step_type'], tf.int32)\n",
        "    reward = tf.convert_to_tensor(data[i]['reward'], tf.float32)\n",
        "    discount = tf.convert_to_tensor(data[i]['discount'], tf.float32)\n",
        "\n",
        "    traj = trajectory.Trajectory(step_type, observation, action, policy_info, next_step_type, reward, discount)\n",
        "    buffer.add_batch(traj)\n",
        "  print('Replay buffer ready')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-fXlWzn4xna",
        "colab_type": "text"
      },
      "source": [
        "The replay buffer, after the function call, is a collection of Trajectories."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9lX70375dQ3",
        "colab_type": "text"
      },
      "source": [
        "The agent needs access to the replay buffer. This is provided by creating an iterable `tf.data.Dataset` pipeline which will feed data to the agent.\n",
        "\n",
        "Each row of the replay buffer only stores a single observation step. But since the DQN Agent needs both the current and next observation to compute the loss, the dataset pipeline will sample two adjacent rows for each item in the batch (`num_steps=2`).\n",
        "\n",
        "This dataset is also optimized by running parallel calls and prefetching data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBPXjC6qdeXR",
        "colab_type": "text"
      },
      "source": [
        "### Metrics for Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgmyJ1keeHJc",
        "colab_type": "text"
      },
      "source": [
        "The most common metric used to evaluate a policy is the average return. The return is the sum of rewards obtained while running a policy in an environment for an episode. Several episodes are run, creating an average return.\n",
        "\n",
        "The following function computes the average return of a policy, given the policy, environment, and a number of episodes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xl2r9yx4dp0S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@test {\"skip\": true}\n",
        "def compute_avg_return(environment, policy, num_episodes=10):\n",
        "\n",
        "  \"\"\"\n",
        "  Calculates the average return \n",
        "  Arguments:\n",
        "      environment : environment used for the agent to be evaluated\n",
        "      policy : the policy, that the agent is using, to be evaluated\n",
        "      num_episodes : how many episodes should the policy be evaluated for\n",
        "  Returns:\n",
        "      The evaluation average return\n",
        "  \"\"\"\n",
        "\n",
        "  total_return = 0.0\n",
        "  for _ in range(num_episodes):\n",
        "\n",
        "    time_step = environment.reset()\n",
        "    episode_return = 0.0\n",
        "\n",
        "    while not time_step.is_last():\n",
        "      action_step = policy.action(time_step)\n",
        "      time_step = environment.step(action_step.action)\n",
        "      episode_return += time_step.reward\n",
        "    total_return += episode_return\n",
        "\n",
        "  avg_return = total_return / num_episodes\n",
        "  return avg_return.numpy()[0]\n",
        "\n",
        "\n",
        "# See also the metrics module for standard implementations of different metrics.\n",
        "# https://github.com/tensorflow/agents/tree/master/tf_agents/metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqK6pw5gHVml",
        "colab_type": "text"
      },
      "source": [
        "For each evaluation we save a new policy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RVeFeOl_fY0f",
        "colab": {}
      },
      "source": [
        "def generate_policy_folder_name(avg, ID):\n",
        "\n",
        "  \"\"\"\n",
        "  Generates new policy folder name to be created in the bucket\n",
        "  Arguments:\n",
        "      avg : policy average return\n",
        "      ID : policy ID \n",
        "  Returns:\n",
        "      The new folder name to be created\n",
        "  \"\"\"\n",
        "\n",
        "  i = datetime.datetime.now() - datetime.datetime(1970,1,1)\n",
        "  folder_name = str(ID) + '.' + str(avg) + '.policy.' + str(i.total_seconds()) \n",
        "  return folder_name"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVRld-_QeRjR",
        "colab_type": "text"
      },
      "source": [
        "### Big Query"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DsEghpAWzp3z",
        "colab": {}
      },
      "source": [
        "def get_last_ID(table_id):\n",
        "\n",
        "  \"\"\"\n",
        "  Query for the last ID in table\n",
        "  Arguments:\n",
        "      table_id : qbq table id used as reference\n",
        "  Returns:\n",
        "      The last integer ID found in the table\n",
        "  \"\"\"\n",
        "\n",
        "  path = project_id + \".\" + table_id\n",
        "  query = \"SELECT * FROM `\" + path + \"` ORDER BY ID DESC LIMIT 1\"\n",
        "  row = pd.read_gbq(query=query, project_id=project_id)\n",
        "  return int(row['ID'][0])\n",
        "\n",
        "def get_last_env_ID():\n",
        "\n",
        "  \"\"\"\n",
        "  Query for the last environment ID\n",
        "  Returns:\n",
        "      The last integer environment ID\n",
        "  \"\"\"\n",
        "\n",
        "  path = project_id + \".\" + episodes_table_id\n",
        "  query = \"SELECT * FROM `\" + path + \"` ORDER BY env_ID DESC LIMIT 1\"\n",
        "  row = pd.read_gbq(query=query, project_id=project_id)\n",
        "  return int(row['env_ID'][0])\n",
        "\n",
        "new_env_ID = 1 + get_last_env_ID()\n",
        "\n",
        "# SQL queries\n",
        "\n",
        "## query for randomly collected trajs\n",
        "query_first_run = \"SELECT \\\n",
        "                    `ml_piscine_bq.steps`.id, obs0, obs1, obs2, obs3, action, reward, \\\n",
        "                    discount, step_type, next_step_type, policy_info \\\n",
        "                    FROM `ml_piscine_bq.observation` \\\n",
        "                    INNER JOIN `ml_piscine_bq.steps` \\\n",
        "                    ON `ml_piscine_bq.observation`.ID = `ml_piscine_bq.steps`.obs_ID \\\n",
        "                    INNER JOIN `ml_piscine_bq.episodes` \\\n",
        "                    ON `ml_piscine_bq.steps`.epi_ID = `ml_piscine_bq.episodes`.ID \\\n",
        "                    WHERE `ml_piscine_bq.episodes`.env_ID = \" + str(new_env_ID) + \" \\\n",
        "                    ORDER BY `ml_piscine_bq.steps`.id \\\n",
        "                    LIMIT 1000\"\n",
        "\n",
        "## query for collect steps generated by last policy ID\n",
        "def query_from_last_pol(pol_ID):\n",
        "\n",
        "  \"\"\"\n",
        "  Query for trajectories generated by last policy ID\n",
        "  Arguments:\n",
        "      pol_ID : policy ID that generated the data wanted\n",
        "  Returns:\n",
        "      query : SQL query string\n",
        "  \"\"\"\n",
        "\n",
        "  query = \"SELECT \\\n",
        "            `ml_piscine_bq.steps`.id, obs0, obs1, obs2, obs3, action, reward, \\\n",
        "            discount, step_type, next_step_type, policy_info \\\n",
        "            FROM `ml_piscine_bq.observation` \\\n",
        "            INNER JOIN `ml_piscine_bq.steps` \\\n",
        "            ON `ml_piscine_bq.observation`.ID = `ml_piscine_bq.steps`.obs_ID \\\n",
        "            INNER JOIN `ml_piscine_bq.episodes` \\\n",
        "            ON `ml_piscine_bq.steps`.epi_ID = `ml_piscine_bq.episodes`.ID \\\n",
        "            INNER JOIN `ml_piscine_bq.agent_policy_v2` \\\n",
        "            ON `ml_piscine_bq.episodes`.ag_pol_ID = `ml_piscine_bq.agent_policy_v2`.ID \\\n",
        "            WHERE `ml_piscine_bq.episodes`.ag_pol_ID = \" + str(pol_ID) + \" \\\n",
        "            ORDER BY `ml_piscine_bq.steps`.ID\"\n",
        "  return query"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbOMleYyp5B8",
        "colab_type": "text"
      },
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8FXVCkqpzpX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#-------------------- Run TensorBoard --------------------\n",
        "\n",
        "# clear tensorboard cache\n",
        "tf.compat.v1.summary.FileWriterCache.clear()\n",
        "\n",
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "train_log_dir = 'logs/cartpole/' + current_time + '/train'\n",
        "train_summary_writer = tf.summary.create_file_writer(train_log_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urtOuh9i9imS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# run tensorboard ui\n",
        "%tensorboard --logdir logs/cartpole/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Z40v_jVLd0Fg"
      },
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0mMUc5nnJSk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tensorboard_ID = 0\n",
        "ag_pol_ID = 0\n",
        "replay_buffer.clear()\n",
        "first_run = True\n",
        "step = 0\n",
        "dataset = replay_buffer.as_dataset(\n",
        "    num_parallel_calls=3, \n",
        "    sample_batch_size=batch_size, \n",
        "    num_steps=2).prefetch(3)\n",
        "iterator = iter(dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGPTFaXEdSeB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "while True:\n",
        "\n",
        "  # Fetch last policy ID\n",
        "  try:\n",
        "    ag_pol_ID = get_last_ID(agent_policy_table_id)\n",
        "  except:\n",
        "    pass\n",
        "  print(\"Policy ID = \", ag_pol_ID)\n",
        "\n",
        "  # Fetch new data\n",
        "  if first_run and ag_pol_ID == 0:\n",
        "    print(\"First Training\")\n",
        "    query = query_first_run\n",
        "    while True:\n",
        "      try:\n",
        "        data_read = pd.read_gbq(query=query, project_id=project_id).transpose()\n",
        "        first_run = False\n",
        "        if not data_read.empty:\n",
        "          break\n",
        "      except:\n",
        "        print(\"Still no Data, waiting...\")\n",
        "        time.sleep(5)\n",
        "        pass\n",
        "  else:\n",
        "    query = query_from_last_pol(ag_pol_ID)\n",
        "    while True:\n",
        "      try:\n",
        "        data_read = pd.read_gbq(query=query, project_id=project_id).transpose()\n",
        "        if not data_read.empty:\n",
        "          break\n",
        "      except:\n",
        "        print(\"Awaiting for new Data\")\n",
        "        time.sleep(5)\n",
        "        pass\n",
        "\n",
        "  # Feed replay buffer and ready dataset for training\n",
        "  add_data_to_replay_buffer(replay_buffer, data_read)\n",
        "\n",
        "  # Train and Eval\n",
        "\n",
        "  ## (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
        "  agent.train = common.function(agent.train)\n",
        "\n",
        "  ## Reset the train step\n",
        "  agent.train_step_counter.assign(0)\n",
        "\n",
        "  ## Evaluate the agent's policy once before training.\n",
        "  avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "  print ('Evaluate the agent\\'s policy once before training - ' ,avg_return)\n",
        "  returns = [avg_return]\n",
        "  try:\n",
        "    print('Last best average return = ', best_avg)\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "  print('--------------------------------------')\n",
        "  for _ in range(num_iterations + 1):\n",
        "\n",
        "    step = agent.train_step_counter.numpy()\n",
        "\n",
        "    # Sample a batch of data from the buffer and update the agent's network.\n",
        "    experience, unused_info = next(iterator)\n",
        "    train_loss = agent.train(experience).loss\n",
        "\n",
        "    if step % 5 == 0:\n",
        "      print('step = {0}: loss = {1}'.format(step, train_loss))\n",
        "\n",
        "    # Evaluate\n",
        "    if step % eval_interval == 0 and step != 0:\n",
        "      avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "      print('\\nstep = {0}: Average Return = {1}'.format(step, avg_return))\n",
        "\n",
        "      # Output to Tensorboard\n",
        "      with train_summary_writer.as_default():\n",
        "        tf.summary.scalar('avg_return', avg_return, step=tensorboard_ID)\n",
        "        tensorboard_ID += 1\n",
        "\n",
        "      # Save policy to drive\n",
        "      ag_pol_ID += 1\n",
        "      saver = policy_saver.PolicySaver(agent.collect_policy, batch_size=None)\n",
        "      policy_folder_name = generate_policy_folder_name(avg_return, ag_pol_ID)\n",
        "      saver.save(policy_folder_name)\n",
        "      \n",
        "      # Upload Policy\n",
        "      path = policy_bucket_folder + policy_folder_name\n",
        "      ag_pol_d[0] = {\n",
        "          'ID': ag_pol_ID,\n",
        "          'source': path,\n",
        "          'avg_return': avg_return,\n",
        "          'eval_episodes': num_eval_episodes,\n",
        "          'training_steps': num_iterations,\n",
        "          'optimizer': str(optimizer).split(' ')[0][1:],\n",
        "          'loss_fn': str(loss_fn).split(' ')[1],\n",
        "          'learning_rate': learning_rate,\n",
        "          'dqn_layer_params': str(dqn_layer_params)\n",
        "      }\n",
        "      ## To GQB\n",
        "      df_ag_pol = pd.DataFrame(ag_pol_d).transpose()\n",
        "      df_ag_pol.to_gbq(agent_policy_table_id, if_exists='append', project_id=project_id, table_schema=agent_policy_schema)\n",
        "      print()\n",
        "      ## To Bucket\n",
        "      !gsutil -q cp -r $policy_folder_name $path\n",
        "      !rm -r $policy_folder_name\n",
        "\n",
        "  print('--------------------------------------')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}