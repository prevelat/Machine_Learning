{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BQ_DataCollection_documented.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prevelat/Machine_Learning/blob/master/BQ_DataCollection_documented.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1u9QVVsShC9X"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjA9YdbQ_Uu6",
        "colab_type": "text"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KEHR2Ui-lo8O",
        "colab": {}
      },
      "source": [
        "# Note: If you haven't installed the following dependencies, run:\n",
        "!apt-get install -y xvfb\n",
        "!pip install 'gym==0.10.11'\n",
        "!pip install 'imageio==2.4.0'\n",
        "!pip install PILLOW\n",
        "!pip install 'pyglet==1.3.2'\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install tf-agents\n",
        "!pip install tensorflow==2.0.0\n",
        "!pip install tensorflow-probability==0.8\n",
        "try:\n",
        "  %%tensorflow_version 2.x\n",
        "except:\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sMitx5qSgJk1",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import base64\n",
        "import imageio\n",
        "import IPython\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL.Image\n",
        "import pyvirtualdisplay\n",
        "import numpy as np\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import time\n",
        "\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.drivers import dynamic_step_driver\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.eval import metric_utils\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.networks import q_network\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.utils import common\n",
        "from tf_agents.policies import policy_saver\n",
        "\n",
        "tf.compat.v1.enable_v2_behavior()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NspmzG4nP3b9",
        "colab": {}
      },
      "source": [
        "tf.version.VERSION"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oreaN8v7_c8G",
        "colab_type": "text"
      },
      "source": [
        "### Cloud Management"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jAuZidP4DZK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Google Cloud Setup\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "try:\n",
        "  os.chdir(\"drive/My Drive/Colab Notebooks\")\n",
        "except:\n",
        "  pass\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "print('Authenticated')\n",
        "\n",
        "from google.cloud import bigquery"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yToU4bbbflRC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Google Cloud Variables\n",
        "\n",
        "project_id = 'ml-piscine-262622'\n",
        "dataset_id = 'ml_piscine_bq'\n",
        "\n",
        "environment_table_id = dataset_id + '.environment'\n",
        "agent_policy_table_id = dataset_id + '.agent_policy_v2'\n",
        "observation_table_id = dataset_id + '.observation'\n",
        "episodes_table_id = dataset_id + '.episodes'\n",
        "steps_table_id = dataset_id + '.steps'\n",
        "traj_raw_table_id = dataset_id + '.traj_raw'\n",
        "\n",
        "client = bigquery.Client(project=project_id)\n",
        "try:\n",
        "  dataset = bigquery.Dataset(dataset_id)\n",
        "  dataset.location = \"US\"\n",
        "  dataset = client.create_dataset(dataset)\n",
        "except:\n",
        "  pass\n",
        "\n",
        "!gcloud config set project {project_id}\n",
        "policy_bucket_folder = 'gs://ml-piscine-bucket/policy/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LmC0NDhdLIKY"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HC1kNrOsLSIZ",
        "colab": {}
      },
      "source": [
        "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
        "\n",
        "batch_size = 64  # @param {type:\"integer\"}\n",
        "learning_rate = 1e-3  # @param {type:\"number\"}\n",
        "\n",
        "collect_steps = 1000  # @param {type:\"integer\"}\n",
        "\n",
        "dqn_layer_params = (100, )  # @param"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOfLj0PSh30T",
        "colab_type": "text"
      },
      "source": [
        "### Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bh1wBwA4zGft",
        "colab_type": "text"
      },
      "source": [
        "Load the CartPole environment from the OpenAI Gym suite. <br/>\n",
        "Usually two environments are instantiated: one for training and one for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pYEz-S9gEv2-",
        "colab": {}
      },
      "source": [
        "env_name = 'CartPole-v0'\n",
        "env = suite_gym.load(env_name)\n",
        "train_py_env = suite_gym.load(env_name)\n",
        "eval_py_env = suite_gym.load(env_name)\n",
        "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
        "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDanNpi4igIg",
        "colab_type": "text"
      },
      "source": [
        "### Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yI1yMqaRiUQ3",
        "colab_type": "text"
      },
      "source": [
        "The DQN agent can be used in any environment which has a discrete action space.\n",
        "\n",
        "At the heart of a DQN Agent is a QNetwork, a neural network model that can learn to predict QValues (expected returns) for all actions, given an observation from the environment.\n",
        "\n",
        "Use tf_agents.networks.q_network to create a QNetwork, passing in the observation_spec, action_spec, and a tuple describing the number and size of the model's hidden layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TgkdEPg_muzV",
        "colab": {}
      },
      "source": [
        "fc_layer_params = dqn_layer_params\n",
        "\n",
        "q_net = q_network.QNetwork(\n",
        "    train_env.observation_spec(),\n",
        "    train_env.action_spec(),\n",
        "    fc_layer_params=fc_layer_params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfi_g3C0iiRE",
        "colab_type": "text"
      },
      "source": [
        "Now use `tf_agents.agents.dqn.dqn_agent` to instantiate a `DqnAgent`. In addition to the `time_step_spec`, `action_spec` and the QNetwork, the agent constructor also requires an optimizer (in this case, `AdamOptimizer`), a loss function, and an integer step counter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jbY4yrjTEyc9",
        "colab": {}
      },
      "source": [
        "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "\n",
        "train_step_counter = tf.Variable(0)\n",
        "\n",
        "agent = dqn_agent.DqnAgent(\n",
        "    train_env.time_step_spec(),\n",
        "    train_env.action_spec(),\n",
        "    q_network=q_net,\n",
        "    optimizer=optimizer,\n",
        "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
        "    train_step_counter=train_step_counter)\n",
        "\n",
        "agent.initialize()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtKjnwh7OicG",
        "colab_type": "text"
      },
      "source": [
        "### Policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvD_UYRI2Cit",
        "colab_type": "text"
      },
      "source": [
        "Download last Policy saved"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUmCkvccOlzN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_policy(ID):\n",
        "\n",
        "  \"\"\"\n",
        "  Reach for the Bucket and update policy based on it's ID\n",
        "  Arguments:\n",
        "      ID : ID of the policy to be downloaded\n",
        "  Returns:\n",
        "      Agent policy and it's average return for reference\n",
        "  \"\"\"\n",
        "\n",
        "  print('-----------------------FETCHING POLICY')\n",
        "  query = \"SELECT `\" + agent_policy_table_id + \"`.source \\\n",
        "            FROM `\" + agent_policy_table_id + \"` \\\n",
        "            ORDER BY ID DESC LIMIT 1\"\n",
        "  path = pd.read_gbq(query=query, project_id=project_id).transpose().to_numpy()[0][0]\n",
        "  folder_name = path.split('/')[-1]\n",
        "  avg_ret = float(folder_name.split('.')[1] + '.' + folder_name.split('.')[2])\n",
        "  !mkdir $folder_name\n",
        "  path = policy_bucket_folder + folder_name\n",
        "  print('--------DOWNLOADING POLICY FROM BUCKET')\n",
        "  print('--------------------------------------')\n",
        "  print('--------------------------------------')\n",
        "  !gsutil cp -r $path $folder_name\n",
        "  print('--------------------------------------')\n",
        "  print('--------------------------------------')\n",
        "  sub_folder = folder_name + '/' + folder_name\n",
        "  policy = tf.compat.v2.saved_model.load(sub_folder)\n",
        "  !rm -r $folder_name\n",
        "\n",
        "  return policy, avg_ret"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiAuM0fRjK_2",
        "colab_type": "text"
      },
      "source": [
        "### Replay Buffer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PA8ecbe0zC-S",
        "colab_type": "text"
      },
      "source": [
        "The replay buffer keeps track of data collected from the environment. This tutorial uses `tf_agents.replay_buffers.tf_uniform_replay_buffer.TFUniformReplayBuffer`, as it is the most common. \n",
        "\n",
        "The constructor requires the specs for the data it will be collecting. This is available from the agent using the `collect_data_spec` method. The batch size and maximum buffer length are also required."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vX2zGUWJGWAl",
        "colab": {}
      },
      "source": [
        "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "    data_spec=agent.collect_data_spec,\n",
        "    batch_size=train_env.batch_size,\n",
        "    max_length=replay_buffer_max_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CA0XPccljQ3T",
        "colab_type": "text"
      },
      "source": [
        "For most agents, collect_data_spec is a named tuple called Trajectory, containing the specs for observations, actions, rewards, and other items."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_IZ-3HcqgE1z",
        "colab": {}
      },
      "source": [
        "agent.collect_data_spec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sy6g1tGcfRlw",
        "colab": {}
      },
      "source": [
        "agent.collect_data_spec._fields"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZZhJPOljXmg",
        "colab_type": "text"
      },
      "source": [
        "### Data Collection fn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wr1KSAEGG4h9",
        "colab": {}
      },
      "source": [
        "def dict_from_traj(traj):\n",
        "\n",
        "  \"\"\"\n",
        "  Serialize trajectory tensor to dict\n",
        "  Arguments:\n",
        "      traj : trajectory tensor\n",
        "  Returns:\n",
        "      d : dict containing the serialized tensor\n",
        "  \"\"\"\n",
        "\n",
        "  d = {\n",
        "    'step_type': traj[0].numpy()[0],\n",
        "    'observation': traj[1].numpy()[0],\n",
        "    'action': traj[2].numpy()[0],\n",
        "    'next_step_type': traj[4].numpy()[0],\n",
        "    'reward': traj[5].numpy()[0],\n",
        "    'discount': traj[6].numpy()[0],\n",
        "    'datetime': datetime.datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\")\n",
        "  }\n",
        "  try:\n",
        "    d['policy_info'] = traj[3].numpy()\n",
        "  except:\n",
        "    d['policy_info'] = traj[3]\n",
        "  return d\n",
        "\n",
        "def collect_step(environment, policy, data_dict, i):\n",
        "\n",
        "  \"\"\"\n",
        "  Having the policy as reference generate 1 step inside the environment and add it to the dict\n",
        "  Arguments:\n",
        "      environment : environment where data is going to be collected\n",
        "      policy : agent policy to be used for collection\n",
        "      data_dict : where the new step collected should be stored\n",
        "      i : step counter\n",
        "  \"\"\"\n",
        "\n",
        "  time_step = environment.current_time_step()\n",
        "  action_step = policy.action(time_step)\n",
        "  next_time_step = environment.step(action_step.action)\n",
        "  traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
        "  data_dict[i] = dict_from_traj(traj)\n",
        "\n",
        "def collect_data(env, policy, steps):\n",
        "\n",
        "  \"\"\"\n",
        "  Generate a dict containing new data collected\n",
        "  Arguments:\n",
        "      env : environment where data is going to be collected\n",
        "      policy : agent policy to be used for collection\n",
        "      steps : how many steps should be collected\n",
        "  Returns:\n",
        "      data_dict : the dict containing all new data generated\n",
        "  \"\"\"\n",
        "\n",
        "  data_dict = dict()\n",
        "  for i in range(steps):\n",
        "    collect_step(env, policy, data_dict, i)\n",
        "  return data_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBi2Zs1JH8R1",
        "colab_type": "text"
      },
      "source": [
        "### GBQ fn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iTXwK_ZCunL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "steps_schema = [\n",
        "  {'name': 'ID', 'type': 'INTEGER', 'mode': 'REQUIRED'},\n",
        "  {'name': 'obs_ID', 'type': 'INTEGER', 'mode': 'REQUIRED'},\n",
        "  {'name': 'epi_ID', 'type': 'INTEGER', 'mode': 'REQUIRED'},\n",
        "  {'name': 'order_in_epi', 'type': 'INTEGER', 'mode': 'REQUIRED'},\n",
        "  {'name': 'action', 'type': 'INTEGER', 'mode': 'REQUIRED'},\n",
        "  {'name': 'reward', 'type': 'FLOAT', 'mode': 'REQUIRED'},\n",
        "  {'name': 'discount', 'type': 'FLOAT', 'mode': 'REQUIRED'},\n",
        "  {'name': 'step_type', 'type': 'INTEGER', 'mode': 'REQUIRED'},\n",
        "  {'name': 'next_step_type', 'type': 'INTEGER', 'mode': 'REQUIRED'},\n",
        "  {'name': 'policy_info', 'type': 'STRING', 'mode': 'REQUIRED'}\n",
        "]\n",
        "\n",
        "observation_schema = [\n",
        "  {'name': 'ID', 'type': 'INTEGER', 'mode': 'REQUIRED'},\n",
        "  {'name': 'obs0', 'type': 'FLOAT', 'mode': 'REQUIRED'},\n",
        "  {'name': 'obs1', 'type': 'FLOAT', 'mode': 'REQUIRED'},\n",
        "  {'name': 'obs2', 'type': 'FLOAT', 'mode': 'REQUIRED'},\n",
        "  {'name': 'obs3', 'type': 'FLOAT', 'mode': 'REQUIRED'},\n",
        "]\n",
        "\n",
        "agent_policy_schema = [\n",
        "  {'name': 'ID', 'type': 'INTEGER', 'mode': 'REQUIRED'},\n",
        "  {'name': 'source', 'type': 'STRING', 'mode': 'REQUIRED'},\n",
        "  {'name': 'avg_return', 'type': 'FLOAT', 'mode': 'REQUIRED'},\n",
        "  {'name': 'eval_episodes', 'type': 'INTEGER', 'mode': 'REQUIRED'},\n",
        "  {'name': 'training_steps', 'type': 'INTEGER', 'mode': 'REQUIRED'},\n",
        "  {'name': 'optimizer', 'type': 'STRING', 'mode': 'REQUIRED'},\n",
        "  {'name': 'loss_fn', 'type': 'STRING', 'mode': 'REQUIRED'},\n",
        "  {'name': 'learning_rate', 'type': 'FLOAT', 'mode': 'REQUIRED'},\n",
        "  {'name': 'dqn_layer_params', 'type': 'STRING', 'mode': 'REQUIRED'}\n",
        "]\n",
        "\n",
        "episodes_schema = [\n",
        "  {'name': 'ID', 'type': 'INTEGER', 'mode': 'REQUIRED'},\n",
        "  {'name': 'ag_pol_ID', 'type': 'INTEGER', 'mode': 'REQUIRED'},\n",
        "  {'name': 'env_ID', 'type': 'INTEGER', 'mode': 'REQUIRED'},\n",
        "  {'name': 'score', 'type': 'FLOAT', 'mode': 'REQUIRED'},\n",
        "  {'name': 'datetime', 'type': 'STRING', 'mode': 'REQUIRED'}\n",
        "]\n",
        "\n",
        "environment_schema = [\n",
        "  {'name': 'ID', 'type': 'INTEGER', 'mode': 'REQUIRED'},\n",
        "  {'name': 'env_name', 'type': 'STRING', 'mode': 'REQUIRED'},\n",
        "  {'name': 'source', 'type': 'STRING', 'mode': 'REQUIRED'},\n",
        "  {'name': 'collect_steps', 'type': 'INTEGER', 'mode': 'REQUIRED'}\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNQvkH7wtU2v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_last_ID(table_id):\n",
        "\n",
        "  \"\"\"\n",
        "  Query for the last ID in table\n",
        "  Arguments:\n",
        "      table_id : qbq table id used as reference\n",
        "  Returns:\n",
        "      The last integer ID found in the table\n",
        "  \"\"\"\n",
        "\n",
        "  path = project_id + \".\" + table_id\n",
        "  query = \"SELECT * FROM `\" + path + \"` ORDER BY ID DESC LIMIT 1\"\n",
        "  row = pd.read_gbq(query=query, project_id=project_id)\n",
        "  return int(row['ID'][0])\n",
        "\n",
        "def get_IDs():\n",
        "\n",
        "  \"\"\"\n",
        "  Query for the last agent_policy, steps, observation and episode table IDs so no data is overlapped\n",
        "  Returns:\n",
        "      Each last ID found within each table\n",
        "  \"\"\"\n",
        "\n",
        "  ag_pol_ID = 0\n",
        "  try:\n",
        "    ag_pol_ID = get_last_ID(agent_policy_table_id)\n",
        "  except:\n",
        "    pass\n",
        "  steps_ID = 0\n",
        "  try:\n",
        "    steps_ID = 1 + get_last_ID(steps_table_id)\n",
        "  except:\n",
        "    pass\n",
        "  obs_ID = 0\n",
        "  try:\n",
        "    obs_ID = 1 + get_last_ID(observation_table_id)\n",
        "  except:\n",
        "    pass\n",
        "  epi_ID = 0\n",
        "  try:\n",
        "    epi_ID = 1 + get_last_ID(episodes_table_id)\n",
        "  except:\n",
        "    pass\n",
        "  return ag_pol_ID, steps_ID, obs_ID, epi_ID"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0otwxcx06uo",
        "colab_type": "text"
      },
      "source": [
        "Upload environment data to Big Query"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAXOcKyh054R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env_d = dict()\n",
        "\n",
        "env_ID = 0\n",
        "try:\n",
        "  env_ID = 1 + get_last_ID(environment_table_id)\n",
        "except:\n",
        "  pass\n",
        "\n",
        "env_d[0] = {\n",
        "    'ID': env_ID,\n",
        "    'env_name': env_name,\n",
        "    'source': str(\"gym\"),\n",
        "    'collect_steps': int(collect_steps)\n",
        "}\n",
        "\n",
        "df_env = pd.DataFrame(env_d).transpose()\n",
        "df_env.to_gbq(environment_table_id, if_exists='append', project_id=project_id, table_schema=environment_schema)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoWvXBqxyrlR",
        "colab_type": "text"
      },
      "source": [
        "## Data Collection loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcGKihrynRpP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "first_run = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQX8wdcFlkq2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "while True:\n",
        "\n",
        "  ag_pol_ID, steps_ID, obs_ID, epi_ID = get_IDs()\n",
        "\n",
        "  # Get Policy\n",
        "  if ag_pol_ID == 0 and first_run:\n",
        "    first_run = False\n",
        "    policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(), train_env.action_spec())\n",
        "    avg = 0\n",
        "  else:\n",
        "    while True:\n",
        "      if ag_pol_ID == 0:\n",
        "        print(\"Awaiting new Policy\")\n",
        "        time.sleep(15)\n",
        "        try:\n",
        "          ag_pol_ID = get_last_ID(agent_policy_table_id)\n",
        "        except:\n",
        "          pass      \n",
        "      else:\n",
        "        try:\n",
        "          policy, avg = get_policy(ag_pol_ID)\n",
        "          break\n",
        "        except:\n",
        "          pass\n",
        "\n",
        "  print(\"ag_pol = \" + str(ag_pol_ID))\n",
        "  print(\"steps = \" + str(steps_ID))\n",
        "  print(\"obs = \" + str(obs_ID))\n",
        "  print(\"epi = \" + str(epi_ID))\n",
        "  print(\"env = \" + str(env_ID))\n",
        "\n",
        "  # Data Collection\n",
        "  print('----------COLLECTING DATA WITH EVALUATION AVG = ', avg)\n",
        "  data = collect_data(train_env, policy, steps=collect_steps)\n",
        "  print('----------DATA COLLECTED')\n",
        "\n",
        "  # update tables\n",
        "  step_d = dict()\n",
        "  obs_d = dict()\n",
        "  epi_d = dict()\n",
        "  order_in_epi = 0\n",
        "  score = 0\n",
        "\n",
        "  for i in data:\n",
        "    obs_d[i] = {\n",
        "        'ID': obs_ID,\n",
        "        'obs0': float(data[i]['observation'][0]),\n",
        "        'obs1': float(data[i]['observation'][1]),\n",
        "        'obs2': float(data[i]['observation'][2]),\n",
        "        'obs3': float(data[i]['observation'][3])\n",
        "    }\n",
        "    step_d[i] = {\n",
        "        'ID': steps_ID,\n",
        "        'obs_ID': obs_ID,\n",
        "        'epi_ID': epi_ID,\n",
        "        'order_in_epi': order_in_epi,\n",
        "        'action': int(data[i]['action']),\n",
        "        'reward': float(data[i]['reward']),\n",
        "        'discount': float(data[i]['discount']),\n",
        "        'step_type': int(data[i]['step_type']),\n",
        "        'next_step_type': int(data[i]['next_step_type']),\n",
        "        'policy_info': str(data[i]['policy_info'])\n",
        "    }\n",
        "    order_in_epi += 1\n",
        "    score += data[i]['reward']\n",
        "    if (data[i]['step_type'] == 2):\n",
        "      epi_d[i] = {\n",
        "          'ID': epi_ID,\n",
        "          'ag_pol_ID': ag_pol_ID,\n",
        "          'env_ID': env_ID,\n",
        "          'score': float(score),\n",
        "          'datetime': data[i]['datetime']\n",
        "      }\n",
        "      order_in_epi = 0\n",
        "      epi_ID += 1\n",
        "      score = 0\n",
        "    obs_ID += 1\n",
        "    steps_ID += 1\n",
        "\n",
        "  df_obs = pd.DataFrame(obs_d).transpose()\n",
        "  df_step = pd.DataFrame(step_d).transpose()\n",
        "  df_epi = pd.DataFrame(epi_d).transpose()\n",
        "\n",
        "  df_obs.to_gbq(observation_table_id, if_exists='append', project_id=project_id, table_schema=observation_schema)\n",
        "  df_step.to_gbq(steps_table_id, if_exists='append', project_id=project_id, table_schema=steps_schema)\n",
        "  df_epi.to_gbq(episodes_table_id, if_exists='append', project_id=project_id, table_schema=episodes_schema)\n",
        "\n",
        "\n",
        "  print(\"\\nCOLLECTION COMPLETE - DATA TABLE UPDATED\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}